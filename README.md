# LLM Optimization Tutorial

A beginner-friendly, hands-on tutorial for learning Large Language Model (LLM) training optimization techniques.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## About The Project

This project provides a minimal, easy-to-understand codebase for fine-tuning Large Language Models. Our core philosophy is to explain complex optimization techniques with the simplest possible code.

To achieve this, **we use only foundational PyTorch and avoid high-level abstractions like Hugging Face `Trainer` or PyTorch Lightning**. This approach allows us to see exactly how optimization strategies are implemented, line by line.

We will start with the [Qwen3](https://huggingface.co/Qwen) series as our example model and build everything from the ground up.


## Features

* **Native PyTorch Focus**: Learn optimization mechanics directly in PyTorch.
* **Maximum Simplicity**: Every chapters is written to be as clear and readable as possible, focusing only on the essential logic.
* **From-Scratch Implementation**: We build the core training components ourselves, giving you a deeper understanding of the entire pipeline.
* **Hands-On & Extensible**: Provides runnable code that you can easily modify and extend for your own experiments.


## Contributing

Contributions are welcome! If you find a bug, have an idea, or want to add an explanation, please feel free to open an issue or submit a pull request.

